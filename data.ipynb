{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T21:54:44.143756Z",
     "start_time": "2019-09-11T21:54:44.013464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from u import *\n",
    "from ut import *\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        self.counter = Counter()\n",
    "        self.total = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        token_id = self.word2idx[word]\n",
    "        self.counter[token_id] += 1\n",
    "        self.total += 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r') as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        return ids\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T21:45:45.642264Z",
     "start_time": "2019-09-11T21:45:19.554071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103227021\n",
      "217646\n",
      "245569\n"
     ]
    }
   ],
   "source": [
    "wik = Data / 'wikitext-103'\n",
    "vocab = Counter()\n",
    "for x in 'train', 'valid', 'test':\n",
    "    text = (wik / x + '.txt').load()\n",
    "    tokens = text.split(' ')\n",
    "    tokens.pop() # remove trailing \\n\n",
    "    vocab.update(tokens)\n",
    "    print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T21:48:51.540828Z",
     "start_time": "2019-09-11T21:48:51.254580Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = vocab.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T22:22:35.790092Z",
     "start_time": "2019-09-11T22:01:58.453296Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = Corpus(wik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T00:26:10.787095Z",
     "start_time": "2019-09-12T00:26:10.704894Z"
    }
   },
   "outputs": [],
   "source": [
    "data = dict(train=from_torch(corpus.train), valid=from_torch(corpus.valid), test=from_torch(corpus.test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T00:27:23.114329Z",
     "start_time": "2019-09-12T00:27:22.571018Z"
    }
   },
   "outputs": [],
   "source": [
    "idx2word = np.array(corpus.dictionary.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T00:28:23.606343Z",
     "start_time": "2019-09-12T00:28:22.940235Z"
    }
   },
   "outputs": [],
   "source": [
    "(Cache.mk() / 'vocab.npy').save(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T00:29:35.743600Z",
     "start_time": "2019-09-12T00:29:35.228415Z"
    }
   },
   "outputs": [],
   "source": [
    "for k in data:\n",
    "    data[k] = data[k].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T00:30:53.500769Z",
     "start_time": "2019-09-12T00:30:49.401227Z"
    }
   },
   "outputs": [],
   "source": [
    "for k in data:\n",
    "    ((Cache / 'wikitext-103').mk() / k + '.npy').save(data[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T00:31:15.267146Z",
     "start_time": "2019-09-12T00:31:11.629629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2, ..., 15,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Cache / 'wikitext-103/train.npy').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T22:54:41.966726Z",
     "start_time": "2019-09-11T22:54:41.910191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267735"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.dictionary.word2idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
