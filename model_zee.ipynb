{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T00:56:44.283653Z",
     "start_time": "2019-10-10T00:56:36.695259Z"
    }
   },
   "outputs": [],
   "source": [
    "from u import *\n",
    "from ut import *\n",
    "from model import *\n",
    "from data import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "decoder = (Cache / 'vocab.npy').load()\n",
    "encoder = get_encoder(decoder)\n",
    "n_vocab = len(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Configurations\n",
    "\n",
    "Also prints out the command to run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T07:48:49.879124Z",
     "start_time": "2019-10-08T07:48:49.756963Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# base hyperparameters for transformer\n",
    "transformer = dict(\n",
    "    model=Proj / 'main.py', model_class='Transformer', n_vocab=n_vocab, step_save=5000,\n",
    "    train_batch=17, train_chunk=1088,\n",
    "    step_eval=500, eval_batch=1, eval_chunk=4096,\n",
    "    cutoffs=[5000, 25000, 50000], adaptive_ratio=4, pos_emb='trained',\n",
    "    n_seq=64, n_layers=16, n_embed=256, n_head=8, n_k=32, n_v=32, n_inner=1024, dropout=0.1,\n",
    "    lr=0.0005, step_warmup=100, scheduler='cosine'\n",
    ")\n",
    "\n",
    "# create config object from dictionary\n",
    "c = Config(Wiki / 'hebbian,large', # first argument to config is the path of the folder to create for the run\n",
    "    transformer,\n",
    "    hebbian=True, hebbian_gamma=0.002, hebbian_T=2500,\n",
    "    train_batch=8, train_chunk=1152,\n",
    "    n_embed=512, n_seq=128\n",
    ") # save to the folder\n",
    "\n",
    "# # print out command to run the training\n",
    "# print(c.train(env_gpu=lrange(4), steps=200000, opt='O1'))\n",
    "\n",
    "c = Config(Wiki / 'hebbian,large2', transformer,\n",
    "    hebbian=True, hebbian_gamma=0.002, hebbian_T=2500,\n",
    "    train_batch=7, train_chunk=1152,\n",
    "    n_embed=512, n_k=64, n_v=64, n_seq=96, n_inner=1536\n",
    ")\n",
    "# print(c.train(env_gpu=lrange(6), steps=200000, opt='O1'))\n",
    "\n",
    "c = Config(Wiki / 'hebbian', transformer,\n",
    "           hebbian=True, hebbian_gamma=0.01, hebbian_T=500)\n",
    "# print(c.train(env_gpu=2, steps=200000, opt='O1'))\n",
    "\n",
    "layers = []\n",
    "for i in range(transformer['n_layers']):\n",
    "    if i < 5:\n",
    "        layers.append(dict(lc_kernel_size=3))\n",
    "    elif i < 10:\n",
    "        layers.append(dict(lc_kernel_size=7))\n",
    "    else:\n",
    "        layers.append(dict(lc_kernel_size=15))\n",
    "c = Config(Wiki / 'transformer,lightconv', transformer, train_batch=10,\n",
    "           light_conv=True, layers=layers,\n",
    "           hebbian=True, hebbian_gamma=0.01, hebbian_T=500)\n",
    "# print(c.train(env_gpu=1, steps=200000, opt='O0'))\n",
    "\n",
    "c = Config(Wiki / 'gru',\n",
    "    model=Proj / 'model.py', model_class='RNN', net='GRU', n_vocab=n_vocab, step_save=5000,\n",
    "    train_batch=11, n_seq=2048, step_eval=1000, eval_batch=1, eval_chunk=8192,\n",
    "    cutoffs=[5000, 25000, 50000], adaptive_ratio=4,\n",
    "    lr=0.01, step_warmup=100, scheduler='rsqrt',\n",
    "    num_layers=1, n_embed=512, n_hidden=2048, dropout=0.1,\n",
    "    hebbian=True, hebbian_gamma=0.01, hebbian_T=500\n",
    ")\n",
    "# print(c.train(env_gpu=1, steps=200000))\n",
    "\n",
    "sorted_hebbian = transformer.copy()\n",
    "sorted_hebbian.update(dict(\n",
    "    hebbian=True, hebbian_gamma=0.01, hebbian_T=500,\n",
    "    vocab_sorted=True, cutoffs=[3500, 25000], n_embeds=[256, 64, 4]\n",
    "))\n",
    "c = Config(Wiki / 'sorted,hebbian', sorted_hebbian)\n",
    "# print(c.train(env_gpu=2, steps=200000, opt='O1'))\n",
    "\n",
    "sorted_hebbian_compound = sorted_hebbian.copy()\n",
    "layers = []\n",
    "for i in range(sorted_hebbian_compound['n_layers']):\n",
    "    # more heads when shallow, less heads when deep\n",
    "    # smaller inner when shallow\n",
    "    if i < 4:\n",
    "        layer = dict(n_inner=64, n_head=8, n_k=16, n_v=16)\n",
    "    elif i < 8:\n",
    "        layer = dict(n_inner=256, n_head=4, n_k=32, n_v=32)\n",
    "    elif i < 10:\n",
    "        layer = dict(n_inner=1024, n_head=4, n_k=64, n_v=64)\n",
    "    elif i < 12:\n",
    "        layer = dict(n_inner=1024, n_head=4, n_k=64, n_v=64)\n",
    "    elif i < 16:\n",
    "        layer = dict(n_inner=1536, n_head=2, n_k=128, n_v=128)\n",
    "    layers.append(layer)\n",
    "\n",
    "c = Config(Wiki / 'sorted,hebbian,compound', sorted_hebbian_compound, layers=layers, train_batch=18)\n",
    "# print(c.train(env_gpu=0, steps=200000, opt='O1'))\n",
    "\n",
    "sorted_hebbian_layer12 = sorted_hebbian.copy()\n",
    "sorted_hebbian_layer12.update(dict(n_layers=12, train_batch=18))\n",
    "c = Config(Wiki / 'sorted,hebbian,layer12', sorted_hebbian_layer12)\n",
    "\n",
    "sorted_hebbian_large = sorted_hebbian.copy()\n",
    "sorted_hebbian_large.update(dict(\n",
    "    train_batch=7, train_chunk=1152,\n",
    "    n_embeds=[512, 256, 16],\n",
    "    n_embed=512, n_k=64, n_v=64, n_seq=96, n_inner=1536\n",
    "))\n",
    "c = Config(Wiki / 'sorted,hebbian,large', sorted_hebbian_large)\n",
    "# print('S=$HOME/Research/exercises', c.train(env_gpu=lrange(8), steps=200000, opt='O1').split('\\n')[1])\n",
    "\n",
    "sorted_hebbian_softmax = sorted_hebbian.copy()\n",
    "sorted_hebbian_softmax.update(dict(fix_softmax=True, train_batch=16))\n",
    "c = Config(Wiki / 'sorted,hebbian,softmax', sorted_hebbian_softmax)\n",
    "# print(c.train(env_gpu=0, steps=200000, opt='O1'))\n",
    "\n",
    "sorted_hebbian_mask = sorted_hebbian.copy()\n",
    "sorted_hebbian_mask.update(dict(mask_pad=True, fix_softmax=True, train_batch=16))\n",
    "c = Config(Wiki / 'sorted,hebbian,mask', sorted_hebbian_mask)\n",
    "# print(c.train(env_gpu=3, steps=200000, opt='O1'))\n",
    "\n",
    "large = sorted_hebbian_softmax.copy()\n",
    "large.update(dict(\n",
    "    train_batch=7, train_chunk=1152,\n",
    "    n_embeds=[512, 256, 16],\n",
    "    n_embed=512, n_k=64, n_v=64, n_seq=96, n_inner=1536\n",
    "))\n",
    "c = Config(Wiki / 'large', large)\n",
    "# print('S=$HOME/Research/exercises', c.train(env_gpu=lrange(8), steps=200000, opt='O1').split('\\n')[1])\n",
    "\n",
    "tie_layers = sorted_hebbian_mask.copy()\n",
    "tie_layers.update(dict(\n",
    "    tie_layers=True,\n",
    "    train_batch=10,\n",
    "    n_k=64, n_v=64, n_inner=2048,\n",
    "))\n",
    "c = Config(Wiki / 'tie_layers', tie_layers)\n",
    "# print(c.train(env_gpu=1, steps=200000, opt='O1'))\n",
    "\n",
    "tie_layers_4x4 = sorted_hebbian_mask.copy()\n",
    "tie_layers_4x4.update(dict(\n",
    "    tie_layers=[4, 4, 4, 4],\n",
    "    train_batch=16\n",
    "))\n",
    "c = Config(Wiki / 'tie_layers,4x4', tie_layers_4x4)\n",
    "# print(c.train(env_gpu=2, steps=200000, opt='O1'))\n",
    "\n",
    "tie_layers_8x2 = sorted_hebbian_mask.copy()\n",
    "tie_layers_8x2.update(dict(\n",
    "    tie_layers=[2] * 8,\n",
    "    train_batch=16\n",
    "))\n",
    "c = Config(Wiki / 'tie_layers,8x2', tie_layers_8x2)\n",
    "# print(c.train(env_gpu=1, steps=200000, opt='O1'))\n",
    "\n",
    "tie_layers_8x2_drop_02 = tie_layers_8x2.copy()\n",
    "tie_layers_8x2_drop_02.update(dropout=0.2)\n",
    "del tie_layers_8x2_drop_02['mask_pad']\n",
    "c = Config(Wiki / '8x2,drop_02', tie_layers_8x2_drop_02)\n",
    "# print(c.train(env_gpu=0, steps=200000, opt='O1'))\n",
    "\n",
    "tie_layers_8x2_drop_005 = tie_layers_8x2_drop_02.copy()\n",
    "tie_layers_8x2_drop_005.update(dropout=0.05)\n",
    "c = Config(Wiki / '8x2,drop_005', tie_layers_8x2_drop_005)\n",
    "# print(c.train(env_gpu=2, steps=200000, opt='O1'))\n",
    "\n",
    "tie_layers_8x2_drop_0 = tie_layers_8x2_drop_02.copy()\n",
    "tie_layers_8x2_drop_0.update(dropout=0.0)\n",
    "c = Config(Wiki / '8x2,drop_0', tie_layers_8x2_drop_0)\n",
    "# print(c.train(env_gpu=3, steps=200000, opt='O1'))\n",
    "\n",
    "tie_layers_8x2_drop_0_k_24 = tie_layers_8x2_drop_0.copy()\n",
    "tie_layers_8x2_drop_0_k_24.update(n_k=24, n_v=40)\n",
    "c = Config(Wiki / '8x2,drop_0,k_24', tie_layers_8x2_drop_0_k_24)\n",
    "# print(c.train(env_gpu=0, steps=200000, opt='O1'))\n",
    "\n",
    "tie_layers_8x2_drop_0_k_40 = tie_layers_8x2_drop_0.copy()\n",
    "tie_layers_8x2_drop_0_k_40.update(n_k=24, n_v=40)\n",
    "c = Config(Wiki / '8x2,drop_0,k_40', tie_layers_8x2_drop_0_k_40)\n",
    "# print(c.train(env_gpu=1, steps=200000, opt='O1'))\n",
    "\n",
    "tie_layers_8x2_drop_0_attn_32 = tie_layers_8x2_drop_0.copy()\n",
    "tie_layers_8x2_drop_0_attn_32.update(n_seq=32, train_batch=18)\n",
    "c = Config(Wiki / '8x2,drop_0,attn_32', tie_layers_8x2_drop_0_attn_32)\n",
    "# print(c.train(env_gpu=3, steps=200000, opt='O1'))\n",
    "\n",
    "tie_layers_8x2_warmup_10000 = tie_layers_8x2.copy()\n",
    "tie_layers_8x2_warmup_10000.update(step_warmup=10000, lr=0.005)\n",
    "del tie_layers_8x2_warmup_10000['mask_pad']\n",
    "c = Config(Wiki / '8x2,warmup_10000', tie_layers_8x2_warmup_10000)\n",
    "# print(c.train(env_gpu=3, steps=200000, opt='O1'))\n",
    "\n",
    "tie_layers_8x2_warmup_1000 = tie_layers_8x2_warmup_10000.copy()\n",
    "tie_layers_8x2_warmup_1000.update(step_warmup=1000, lr=0.001)\n",
    "c = Config(Wiki / '8x2,warmup_1000', tie_layers_8x2_warmup_1000)\n",
    "# print(c.train(env_gpu=3, steps=200000, opt='O1'))\n",
    "\n",
    "tie_layers_8x2_warmup_1000_drop_0 = tie_layers_8x2_warmup_1000.copy()\n",
    "tie_layers_8x2_warmup_1000_drop_0.update(dropout=0)\n",
    "c = Config(Wiki / '8x2,warmup_1000,drop_0', tie_layers_8x2_warmup_1000_drop_0)\n",
    "# print(c.train(env_gpu=1, steps=200000, opt='O1'))\n",
    "\n",
    "tie_layers_8x2_warmup_1000_drop_0_k_24 = tie_layers_8x2_warmup_1000.copy()\n",
    "tie_layers_8x2_warmup_1000_drop_0_k_24.update(dropout=0, n_k=24, n_v=40)\n",
    "c = Config(Wiki / '8x2,warmup_1000,drop_0,k_24', tie_layers_8x2_warmup_1000_drop_0_k_24)\n",
    "# print(c.train(env_gpu=3, steps=200000, opt='O1'))\n",
    "\n",
    "layers = []\n",
    "for i in range(tie_layers_8x2['n_layers']):\n",
    "    # more heads when shallow, less heads when deep\n",
    "    # smaller inner when shallow\n",
    "    if i < 2:\n",
    "        layer = dict(n_inner=256, n_head=16, n_k=16, n_v=16)\n",
    "    elif i < 4:\n",
    "        layer = dict(n_inner=512, n_head=8, n_k=32, n_v=32)\n",
    "    elif i < 6:\n",
    "        layer = dict(n_inner=1024, n_head=4, n_k=64, n_v=64)\n",
    "    else:\n",
    "        layer = dict(n_inner=2048, n_head=2, n_k=128, n_v=128)\n",
    "    layers.append(layer)\n",
    "tie_layers_8x2_compound = tie_layers_8x2.copy()\n",
    "tie_layers_8x2_compound.update(layers=layers)\n",
    "del tie_layers_8x2_compound['mask_pad']\n",
    "c = Config(Wiki / '8x2,compound', tie_layers_8x2_compound)\n",
    "# print(c.train(env_gpu=1, steps=200000, opt='O1'))\n",
    "\n",
    "layers = []\n",
    "for i in range(tie_layers_8x2_compound['n_layers']):\n",
    "    # smaller inner when shallow\n",
    "    if i < 2:\n",
    "        layer = dict(n_inner=256, n_head=8, n_k=8, n_v=8)\n",
    "    elif i < 4:\n",
    "        layer = dict(n_inner=512, n_head=8, n_k=16, n_v=16)\n",
    "    elif i < 6:\n",
    "        layer = dict(n_inner=1024, n_head=8, n_k=32, n_v=32)\n",
    "    else:\n",
    "        layer = dict(n_inner=2048, n_head=8, n_k=64, n_v=64)\n",
    "    layers.append(layer)\n",
    "tie_layers_8x2_compound2 = tie_layers_8x2_compound.copy()\n",
    "tie_layers_8x2_compound2.update(layers=layers)\n",
    "c = Config(Wiki / '8x2,compound2', tie_layers_8x2_compound2)\n",
    "# print(c.train(env_gpu=1, steps=200000, opt='O1'))\n",
    "\n",
    "tie_layers_8x2_attn_128 = tie_layers_8x2.copy()\n",
    "tie_layers_8x2_attn_128.update(n_seq=128, train_batch=14)\n",
    "del tie_layers_8x2_attn_128['mask_pad']\n",
    "c = Config(Wiki / '8x2,attn_128', tie_layers_8x2_attn_128)\n",
    "# print(c.train(env_gpu=3, steps=200000, opt='O1'))\n",
    "\n",
    "tie_layers_8x2_attn_128_2 = tie_layers_8x2_attn_128.copy()\n",
    "tie_layers_8x2_attn_128_2.update(train_chunk=1024, dropout=0.2, train_batch=15)\n",
    "c = Config(Wiki / '8x2,attn_128_2', tie_layers_8x2_attn_128_2)\n",
    "# print(c.train(env_gpu=0, steps=200000, opt='O1'))\n",
    "\n",
    "tie_layers_8x2_attn_96 = tie_layers_8x2_attn_128.copy()\n",
    "tie_layers_8x2_attn_96.update(n_seq=96, train_batch=15)\n",
    "c = Config(Wiki / '8x2,attn_96', tie_layers_8x2_attn_96)\n",
    "# print(c.train(env_gpu=0, steps=200000, opt='O1'))\n",
    "\n",
    "universal = sorted_hebbian_mask.copy()\n",
    "universal.update(dict(\n",
    "    model_class='UniversalTransformer',\n",
    "    train_batch=10,\n",
    "    n_k=64, n_v=64, n_inner=2048,\n",
    "    threshold=0.99, time_penalty=0.1\n",
    "))\n",
    "c = Config(Wiki / 'universal', universal)\n",
    "# print(c.train(env_gpu=2, steps=200000, opt='O1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T07:59:41.319759Z",
     "start_time": "2019-10-08T07:59:41.063172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd /data/scratch/zxyan/micronet/wikitext-103/shallow,layers_8,cache\n",
      "CUDA_VISIBLE_DEVICES=3 python3 ../../main.py . steps=200000 opt_level=O1\n",
      "cd /data/scratch/zxyan/micronet/wikitext-103/shallow,layers_8,cache_1500\n",
      "CUDA_VISIBLE_DEVICES=1 python3 ../../main.py . steps=200000 opt_level=O1\n",
      "cd /data/scratch/zxyan/micronet/wikitext-103/shallow,layers_8,cache_1000\n",
      "CUDA_VISIBLE_DEVICES=0 python3 ../../main.py . steps=200000 opt_level=O1\n"
     ]
    }
   ],
   "source": [
    "shallow = sorted_hebbian_softmax.copy()\n",
    "shallow.update(\n",
    "    n_layers=4, train_batch=20,\n",
    "    dropout=0,\n",
    "    step_warmup=1000, lr=0.001,\n",
    "    train_chunk=1152, n_seq=96,\n",
    "    n_inner=1536, n_k=32, n_v=64)\n",
    "c = Config(Wiki / 'shallow', shallow).save(True)\n",
    "# print(c.train(env_gpu=0, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_attn_128 = shallow.copy()\n",
    "shallow_attn_128.update(n_seq=128)\n",
    "c = Config(Wiki / 'shallow,attn_128', shallow_attn_128)\n",
    "# print(c.train(env_gpu=3, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_attn_64 = shallow.copy()\n",
    "shallow_attn_64.update(n_seq=64)\n",
    "c = Config(Wiki / 'shallow,attn_64', shallow_attn_64)\n",
    "# print(c.train(env_gpu=0, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_attn_72 = shallow.copy()\n",
    "shallow_attn_72.update(n_seq=72)\n",
    "c = Config(Wiki / 'shallow,attn_72', shallow_attn_72)\n",
    "# print(c.train(env_gpu=0, steps=200000, opt='O1'))\n",
    "\n",
    "layers = []\n",
    "for i in range(shallow['n_layers']):\n",
    "    if i < 2:\n",
    "        layers.append(dict(lc_kernel_size=7))\n",
    "    else:\n",
    "        layers.append(dict(lc_kernel_size=15))\n",
    "c = Config(Wiki / 'shallow,lightconv', shallow,\n",
    "           light_conv=True, layers=layers, train_batch=16)\n",
    "# print(c.train(env_gpu=3, steps=200000, opt='O0'))\n",
    "\n",
    "shallow_lr_005 = shallow.copy()\n",
    "shallow_lr_005.update(lr=0.005)\n",
    "c = Config(Wiki / 'shallow,lr_005', shallow_lr_005)\n",
    "# print(c.train(env_gpu=3, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_lr_01 = shallow.copy()\n",
    "shallow_lr_01.update(lr=0.01)\n",
    "c = Config(Wiki / 'shallow,lr_01', shallow_lr_01)\n",
    "# print(c.train(env_gpu=0, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_lr_002 = shallow.copy()\n",
    "shallow_lr_002.update(lr=0.002)\n",
    "c = Config(Wiki / 'shallow,lr_002', shallow_lr_002)\n",
    "# print(c.train(env_gpu=1, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_layers_5 = shallow.copy()\n",
    "shallow_layers_5.update(n_layers=5, n_inner=1024, n_v=48)\n",
    "c = Config(Wiki / 'shallow,layers_5', shallow_layers_5).save(True)\n",
    "# print(c.train(env_gpu=0, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_layers_3 = shallow.copy()\n",
    "shallow_layers_3.update(n_layers=3, n_inner=2048, n_k=48)\n",
    "c = Config(Wiki / 'shallow,layers_3', shallow_layers_3).save(True)\n",
    "# print(c.train(env_gpu=3, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_layers_6 = shallow.copy()\n",
    "shallow_layers_6.update(n_layers=6, n_inner=1024, n_k=32, n_v=32)\n",
    "c = Config(Wiki / 'shallow,layers_6', shallow_layers_6).save(True)\n",
    "# print(c.train(env_gpu=0, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_layers_8 = shallow.copy()\n",
    "shallow_layers_8.update(n_layers=8, n_inner=768, n_k=24, n_v=24)\n",
    "c = Config(Wiki / 'shallow,layers_8', shallow_layers_8).save(True)\n",
    "# print(c.train(env_gpu=0, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_layers_12 = shallow.copy()\n",
    "shallow_layers_12.update(n_layers=12, n_inner=512, n_k=16, n_v=16)\n",
    "c = Config(Wiki / 'shallow,layers_12', shallow_layers_12).save(True)\n",
    "# print(c.train(env_gpu=0, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_layers_16 = shallow.copy()\n",
    "shallow_layers_16.update(n_layers=16, n_inner=384, n_k=12, n_v=12, train_batch=19)\n",
    "c = Config(Wiki / 'shallow,layers_16', shallow_layers_16)\n",
    "# print(c.train(env_gpu=1, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_layers_8_attn_64 = shallow_layers_8.copy()\n",
    "shallow_layers_8_attn_64.update(n_seq=64)\n",
    "c = Config(Wiki / 'shallow,layers_8,attn_64', shallow_layers_8_attn_64)\n",
    "# print(c.train(env_gpu=1, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_hebbian_1000 = shallow.copy()\n",
    "shallow_hebbian_1000.update(hebbian_T=1000)\n",
    "c = Config(Wiki / 'shallow,hebbian_1000', shallow_hebbian_1000)\n",
    "# print(c.train(env_gpu=0, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_layers_8_gpu_8 = shallow_layers_8.copy()\n",
    "shallow_layers_8_gpu_8.update(hebbian_T=4000)\n",
    "c = Config(Wiki / 'shallow,layers_8,gpu_8', shallow_layers_8_gpu_8).save(True)\n",
    "# print(c.train(env_gpu=lrange(8), steps=200000, opt='O1'))\n",
    "\n",
    "shallow_layers_8_embed_128 = shallow_layers_8.copy()\n",
    "shallow_layers_8_embed_128.update(n_embed=128, n_embeds=[128, 32, 2])\n",
    "c = Config(Wiki / 'shallow,layers_8,embed_128', shallow_layers_8_embed_128).save(True)\n",
    "# print(c.train(env_gpu=0, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_layers_8_cutoff_1000_10000 = shallow_layers_8.copy()\n",
    "shallow_layers_8_cutoff_1000_10000.update(cutoffs=[1000, 10000], train_batch=17)\n",
    "c = Config(Wiki / 'shallow,layers_8,cutoff_1000_10000', shallow_layers_8_cutoff_1000_10000).save(True)\n",
    "# print(c.train(env_gpu=1, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_layers_8_inner_512 = shallow_layers_8.copy()\n",
    "shallow_layers_8_inner_512.update(n_inner=512, n_k=16)\n",
    "c = Config(Wiki / 'shallow,layers_8,inner_512', shallow_layers_8_inner_512).save(True)\n",
    "# print(c.train(env_gpu=0, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_layers_8_gpu_2 = shallow_layers_8.copy()\n",
    "shallow_layers_8_gpu_2.update(hebbian_T=1000)\n",
    "c = Config(Wiki / 'shallow,layers_8,gpu_2', shallow_layers_8_gpu_2)\n",
    "# print(c.save(True).train(env_gpu=[0, 1], steps=200000, opt='O1'))\n",
    "\n",
    "shallow_layers_8_tt = shallow_layers_8.copy()\n",
    "shallow_layers_8_tt.update(\n",
    "    tensor_train=True, train_batch=16,\n",
    "    modes_embed=[4, 16, 4], modes_inner=[8, 12, 8],\n",
    "    ranks_e2i=[4, 16, 16, 4], ranks_i2e=[4, 16, 16, 4]\n",
    ")\n",
    "c = Config(Wiki / 'shallow,layers_8,tt', shallow_layers_8_tt)\n",
    "# print(c.save(True).train(env_gpu=4, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_layers_8_gpu_4 = shallow_layers_8.copy()\n",
    "shallow_layers_8_gpu_4.update(hebbian_T=2000)\n",
    "c = Config(Wiki / 'shallow,layers_8,gpu_4', shallow_layers_8_gpu_4)\n",
    "# print(c.save(True).train(env_gpu=lrange(4), steps=200000, opt='O1'))\n",
    "\n",
    "shallow_layers_8_gpu_3 = shallow_layers_8.copy()\n",
    "shallow_layers_8_gpu_3.update(hebbian_T=1500)\n",
    "c = Config(Wiki / 'shallow,layers_8,gpu_3', shallow_layers_8_gpu_3)\n",
    "# print(c.save(True).train(env_gpu=lrange(3), steps=200000, opt='O1'))\n",
    "\n",
    "shallow_layers_8_cache = shallow_layers_8.copy()\n",
    "shallow_layers_8_cache.update(train_batch=14, use_cache=True, cache_theta_init=0.016, cache_lambda_init=0.07, n_cache=2000)\n",
    "c = Config(Wiki / 'shallow,layers_8,cache', shallow_layers_8_cache)\n",
    "print(c.save(True).train(env_gpu=3, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_layers_8_cache_1500 = shallow_layers_8.copy()\n",
    "shallow_layers_8_cache_1500.update(train_batch=15, use_cache=True, cache_theta_init=0.016, cache_lambda_init=0.07, n_cache=1500)\n",
    "c = Config(Wiki / 'shallow,layers_8,cache_1500', shallow_layers_8_cache_1500)\n",
    "print(c.save(True).train(env_gpu=1, steps=200000, opt='O1'))\n",
    "\n",
    "shallow_layers_8_cache_1000 = shallow_layers_8.copy()\n",
    "shallow_layers_8_cache_1000.update(train_batch=16, use_cache=True, cache_theta_init=0.016, cache_lambda_init=0.07, n_cache=1000)\n",
    "c = Config(Wiki / 'shallow,layers_8,cache_1000', shallow_layers_8_cache_1000)\n",
    "print(c.save(True).train(env_gpu=0, steps=200000, opt='O1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache Parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T19:05:45.371479Z",
     "start_time": "2019-10-08T19:05:39.389759Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loads the model (net and step) for evaluation\n",
    "c = Config(Wiki / 'shallow,layers_8').load()\n",
    "net, step = c.var(device='cuda:3').load_model('max')\n",
    "from model import evaluate\n",
    "data_val = SequentialIterator(c, c.eval_batch, split='valid')\n",
    "perplexity = {}\n",
    "print('Model at step', step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T07:35:22.552886Z",
     "start_time": "2019-10-08T07:34:15.595825Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "thetas = [1e-2, 1e-3, 1e-4]\n",
    "thetas = [1e-1, 5e-2, 2e-2, 1e-2]\n",
    "thetas = [1e-2, 5e-3, 2e-3, 1e-3]\n",
    "thetas = [1e-2, 9e-3, 8e-3, 7e-3, 6e-3, 5e-3, 4e-3, 3e-3, 2e-3, 1e-3]\n",
    "thetas = np.arange(3e-2, 1e-2, -2e-3)\n",
    "# thetas = [0.02, 0.018, 0.016, 0.015, 0.014, 0.012, 0.01]\n",
    "\n",
    "lambdas = [0.05, 0.1, 0.2, 0.3, 0.4]\n",
    "lambdas = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "lambdas = [0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.15, 0.2]\n",
    "lambdas = [0.06, 0.07, 0.08, 0.09]\n",
    "# lambdas = [0.05, 0.055, 0.06, 0.065, 0.07]\n",
    "# lambdas = np.arange(0.03, 0, -0.01)\n",
    "\n",
    "# search over cache parameters\n",
    "for theta in thetas:\n",
    "    for lam in lambdas:\n",
    "        if (theta, lam) in perplexity:\n",
    "            continue\n",
    "        net.loss.cache_keys = net.loss.cache_values = None\n",
    "        perplexity[theta, lam] = evaluate(c.var(use_cache=True, n_cache=1500, cache_theta=theta, cache_lambda=lam), data_val, net)['perplexity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T07:35:22.653708Z",
     "start_time": "2019-10-08T07:35:22.555385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>theta</th>\n",
       "      <th>0.03</th>\n",
       "      <th>0.027999999999999997</th>\n",
       "      <th>0.025999999999999995</th>\n",
       "      <th>0.023999999999999994</th>\n",
       "      <th>0.021999999999999992</th>\n",
       "      <th>0.01999999999999999</th>\n",
       "      <th>0.017999999999999988</th>\n",
       "      <th>0.015999999999999986</th>\n",
       "      <th>0.013999999999999985</th>\n",
       "      <th>0.011999999999999983</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.06</th>\n",
       "      <td>33.295599</td>\n",
       "      <td>33.216659</td>\n",
       "      <td>33.144749</td>\n",
       "      <td>33.084193</td>\n",
       "      <td>33.040807</td>\n",
       "      <td>33.021898</td>\n",
       "      <td>33.036002</td>\n",
       "      <td>33.093076</td>\n",
       "      <td>33.204457</td>\n",
       "      <td>33.382024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.07</th>\n",
       "      <td>33.248082</td>\n",
       "      <td>33.163625</td>\n",
       "      <td>33.086639</td>\n",
       "      <td>33.021961</td>\n",
       "      <td>32.975755</td>\n",
       "      <td>32.955760</td>\n",
       "      <td>32.971164</td>\n",
       "      <td>33.032639</td>\n",
       "      <td>33.152288</td>\n",
       "      <td>33.342913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.08</th>\n",
       "      <td>33.227692</td>\n",
       "      <td>33.137938</td>\n",
       "      <td>33.056195</td>\n",
       "      <td>32.987542</td>\n",
       "      <td>32.938667</td>\n",
       "      <td>32.917706</td>\n",
       "      <td>32.934395</td>\n",
       "      <td>33.000136</td>\n",
       "      <td>33.127739</td>\n",
       "      <td>33.330975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.09</th>\n",
       "      <td>33.229554</td>\n",
       "      <td>33.134659</td>\n",
       "      <td>33.048307</td>\n",
       "      <td>32.975912</td>\n",
       "      <td>32.924448</td>\n",
       "      <td>32.902570</td>\n",
       "      <td>32.920555</td>\n",
       "      <td>32.990413</td>\n",
       "      <td>33.125820</td>\n",
       "      <td>33.341219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "theta       0.030      0.028      0.026      0.024      0.022      0.020  \\\n",
       "lambda                                                                     \n",
       "0.06    33.295599  33.216659  33.144749  33.084193  33.040807  33.021898   \n",
       "0.07    33.248082  33.163625  33.086639  33.021961  32.975755  32.955760   \n",
       "0.08    33.227692  33.137938  33.056195  32.987542  32.938667  32.917706   \n",
       "0.09    33.229554  33.134659  33.048307  32.975912  32.924448  32.902570   \n",
       "\n",
       "theta       0.018      0.016      0.014      0.012  \n",
       "lambda                                              \n",
       "0.06    33.036002  33.093076  33.204457  33.382024  \n",
       "0.07    32.971164  33.032639  33.152288  33.342913  \n",
       "0.08    32.934395  33.000136  33.127739  33.330975  \n",
       "0.09    32.920555  32.990413  33.125820  33.341219  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([[perplexity[theta, lam] for theta in thetas] for lam in lambdas], index=lambdas, columns=thetas)\n",
    "df.index.name = 'lambda'\n",
    "df.columns.name = 'theta'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T07:37:16.104086Z",
     "start_time": "2019-10-08T07:37:14.190178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 3.5233027935028076, 'perplexity': 33.89619592549835, 'time': 1.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = SequentialIterator(c, c.eval_batch, split='test')\n",
    "evaluate(c.var(use_cache=True, n_cache=1500, cache_theta=0.02, cache_lambda=0.09), data_test, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache from Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T00:57:03.909187Z",
     "start_time": "2019-10-10T00:56:44.285620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model at step 20000\n"
     ]
    }
   ],
   "source": [
    "# loads the model (net and step) for evaluation\n",
    "c = Config(Wiki / 'quant_aware', device='cuda:1').load()\n",
    "from model import get_net\n",
    "from main import evaluate\n",
    "# from quantized_model import get_net, evaluate\n",
    "net = get_net(c)\n",
    "net, step = c.init_model(net, step='max', train=False)\n",
    "data_val = SequentialIterator(c, c.eval_batch, split='valid')\n",
    "data_test = SequentialIterator(c, c.eval_batch, split='test')\n",
    "print('Model at step', step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T00:57:09.839554Z",
     "start_time": "2019-10-10T00:57:03.911642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 3.5198779106140137, 'perplexity': 33.780303995504525, 'time': 1.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.loss.cache_keys = net.loss.cache_values = None\n",
    "evaluate(c, data_val, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T00:57:11.699608Z",
     "start_time": "2019-10-10T00:57:09.841443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 3.5439910888671875, 'perplexity': 34.60475460205126, 'time': 2.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.loss.cache_keys = net.loss.cache_values = None\n",
    "evaluate(c, data_test, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distiller",
   "language": "python",
   "name": "distiller"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "183px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
